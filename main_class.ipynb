{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3dced5-1df9-45e0-8856-e9b3026d6edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd37734-c749-48aa-8bae-958d35825c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New JSON file saved at: /Users/mayarios/Desktop/deeplearning/project/cleaned_books_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "books_path = \"/Users/mayarios/Desktop/deeplearning/project/cleaned_books_data.csv\"\n",
    "new_json_path = \"/Users/mayarios/Desktop/deeplearning/project/cleaned_books_data.json\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(books_path)\n",
    "\n",
    "\n",
    "df.to_json(new_json_path, orient=\"records\", indent=4)\n",
    "\n",
    "print(f\"New JSON file saved at: {new_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446f8779-7c2b-4ec1-a7ee-5ef8d9db0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few entries in the JSON file:\n",
      "[\n",
      "    {\n",
      "        \"id\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\\tes/Austen_Jane-Sense_and_Sensibility.xml.gz\\ts1\\ts1\",\n",
      "        \"english\": \"Source: Project GutenbergAudiobook available here\",\n",
      "        \"spanish\": \"Source: Wikisource & librodot.com\",\n",
      "        \"id_english\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"id_spanish\": \"es/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"segment_id\": \"s1\",\n",
      "        \"alignment_id\": \"s1\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\\tes/Austen_Jane-Sense_and_Sensibility.xml.gz\\ts2\\ts2\",\n",
      "        \"english\": \"Sense and Sensibility\",\n",
      "        \"spanish\": \"SENTIDO Y SENSIBILIDAD\",\n",
      "        \"id_english\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"id_spanish\": \"es/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"segment_id\": \"s2\",\n",
      "        \"alignment_id\": \"s2\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\\tes/Austen_Jane-Sense_and_Sensibility.xml.gz\\ts3\\ts3\",\n",
      "        \"english\": \"Jane Austen\",\n",
      "        \"spanish\": \"JANE AUSTEN\",\n",
      "        \"id_english\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"id_spanish\": \"es/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"segment_id\": \"s3\",\n",
      "        \"alignment_id\": \"s3\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\\tes/Austen_Jane-Sense_and_Sensibility.xml.gz\\ts4\\ts4\",\n",
      "        \"english\": \"CHAPTER 1\",\n",
      "        \"spanish\": \"CAPITULO I\",\n",
      "        \"id_english\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"id_spanish\": \"es/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"segment_id\": \"s4\",\n",
      "        \"alignment_id\": \"s4\"\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\\tes/Austen_Jane-Sense_and_Sensibility.xml.gz\\ts5.0\\ts5.0\",\n",
      "        \"english\": \"The family of Dashwood had long been settled in Sussex.\",\n",
      "        \"spanish\": \"La familia Dashwood llevaba largo tiempo afincada en Sussex.\",\n",
      "        \"id_english\": \"en/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"id_spanish\": \"es/Austen_Jane-Sense_and_Sensibility.xml.gz\",\n",
      "        \"segment_id\": \"s5.0\",\n",
      "        \"alignment_id\": \"s5.0\"\n",
      "    }\n",
      "]\n",
      "\n",
      "First English sentence: Source: Project GutenbergAudiobook available here\n",
      "First Spanish sentence: Source: Wikisource & librodot.com\n",
      "\n",
      "------------Original-------------\n",
      "Source: Project GutenbergAudiobook available here\n",
      "------------Encoded-------------\n",
      "[101, 3120, 1024, 2622, 9535, 11029, 19513, 3695, 8654, 2800, 2182, 102]\n",
      "------------Decoded-------------\n",
      "[CLS] source : project gutenbergaudiobook available here [SEP]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "books_path = \"/Users/mayarios/Desktop/deeplearning/project/cleaned_books_data.json\"\n",
    "\n",
    "\n",
    "with open(books_path, 'r') as f:\n",
    "    books = json.load(f)\n",
    "\n",
    "\n",
    "print(\"First few entries in the JSON file:\")\n",
    "print(json.dumps(books[:5], indent=4))\n",
    "\n",
    "\n",
    "first_english_sentence = books[0]['english']\n",
    "first_spanish_sentence = books[0]['spanish']\n",
    "\n",
    "print(\"\\nFirst English sentence:\", first_english_sentence)\n",
    "print(\"First Spanish sentence:\", first_spanish_sentence)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "output = tokenizer(first_english_sentence, truncation=True, max_length=100)\n",
    "\n",
    "print(\"\\n------------Original-------------\")\n",
    "print(first_english_sentence)\n",
    "\n",
    "print(\"------------Encoded-------------\")\n",
    "print(output['input_ids'])\n",
    "\n",
    "print(\"------------Decoded-------------\")\n",
    "print(tokenizer.decode(output['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea9f99b-6792-47c1-b357-8b2718deb3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few entries in the JSON file:\n",
      "[\n",
      "    {\n",
      "        \"english\": \"`White mould' or `Sclerotinia rot' is a very destructive plant disease which can affect agricultural crops such as oilseed rape, soybean and sunflower.\",\n",
      "        \"spanish\": \"El `moho blanco' es una enfermedad de las plantas muy destructiva que puede afectar a cultivos de semillas oleaginosas como colza, soja y girasoles.\"\n",
      "    },\n",
      "    {\n",
      "        \"english\": \"A action-recognition system is then activated: it receives the information from the sensors, compares those with the task model corresponding to the steps of preparing a milk and sugar cup of tea.\",\n",
      "        \"spanish\": \"Entonces se activa un sistema de reconocimiento de acciones: este recibe la informaci\\u00f3n de los sensores y la compara con el modelo de tareas correspondientes a los pasos necesarios para la preparaci\\u00f3n de una taza de t\\u00e9 con leche y az\\u00facar.\"\n",
      "    },\n",
      "    {\n",
      "        \"english\": \"A actions cover demonstration projects implementing innovative energy technologies for rational use of energy, renewable energy sources, and fossil fuels;\",\n",
      "        \"spanish\": \"Las acciones A abarcan proyectos de demostraci\\u00f3n que ponen en pr\\u00e1ctica tecnolog\\u00edas energ\\u00e9ticas innovadoras para el uso racional de la energ\\u00eda, fuentes de energ\\u00eda renovables y combustibles f\\u00f3siles;\"\n",
      "    },\n",
      "    {\n",
      "        \"english\": \"a) Aeronautics and space\",\n",
      "        \"spanish\": \"a) Aeron\\u00e1utica y espacio\"\n",
      "    },\n",
      "    {\n",
      "        \"english\": \"A. africanus lived in southern Africa over two million years ago.\",\n",
      "        \"spanish\": \"El A. africanus vivi\\u00f3 en el sur de \\u00c1frica hace m\\u00e1s de dos millones de a\\u00f1os.\"\n",
      "    }\n",
      "]\n",
      "\n",
      "First English sentence: `White mould' or `Sclerotinia rot' is a very destructive plant disease which can affect agricultural crops such as oilseed rape, soybean and sunflower.\n",
      "First Spanish sentence: El `moho blanco' es una enfermedad de las plantas muy destructiva que puede afectar a cultivos de semillas oleaginosas como colza, soja y girasoles.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70ec77cdc8343699013fcd2758aa505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------Original-------------\n",
      "`White mould' or `Sclerotinia rot' is a very destructive plant disease which can affect agricultural crops such as oilseed rape, soybean and sunflower.\n",
      "------------Encoded-------------\n",
      "[101, 1036, 2317, 9587, 21285, 1005, 2030, 1036, 8040, 3917, 4140, 23309, 18672, 1005, 2003, 1037, 2200, 15615, 3269, 4295, 2029, 2064, 7461, 4910, 8765, 2107, 2004, 20631, 13089, 9040, 1010, 25176, 4783, 2319, 1998, 3103, 14156, 1012, 102]\n",
      "------------Decoded-------------\n",
      "[CLS] ` white mould ' or ` sclerotinia rot ' is a very destructive plant disease which can affect agricultural crops such as oilseed rape, soybean and sunflower. [SEP]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset  \n",
    "\n",
    "\n",
    "news_path = \"/Users/mayarios/Desktop/deeplearning/project/cleaned_news_data.json\"\n",
    "\n",
    "\n",
    "with open(news_path, 'r') as f:\n",
    "    news = json.load(f)\n",
    "\n",
    "# Display the first few entries\n",
    "print(\"First few entries in the JSON file:\")\n",
    "print(json.dumps(news[:5], indent=4))\n",
    "\n",
    "# Extract the first English and Spanish sentence\n",
    "first_entry = news[0]\n",
    "first_english_sentence = first_entry.get('english', 'No English text found')\n",
    "first_spanish_sentence = first_entry.get('spanish', 'No Spanish text found')\n",
    "\n",
    "print(\"\\nFirst English sentence:\", first_english_sentence)\n",
    "print(\"First Spanish sentence:\", first_spanish_sentence)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize a single sentence for demonstration\n",
    "tokenized_output = tokenizer(first_english_sentence, truncation=True, max_length=100)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    encoding = tokenizer(examples[\"english\"], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Tokenize Spanish text separately\n",
    "    spanish_encoding = tokenizer(examples[\"spanish\"], padding=\"max_length\", truncation=True)[\"input_ids\"]\n",
    "\n",
    "    # Ensure labels are included or default to zeros\n",
    "    labels = examples.get(\"label\", [0] * len(examples[\"english\"]))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"],  \n",
    "        \"attention_mask\": encoding[\"attention_mask\"],  \n",
    "        \"spanish\": spanish_encoding,  \n",
    "        \"labels\": labels  \n",
    "    }\n",
    "\n",
    "# Convert the news data into a Hugging Face Dataset for mapping\n",
    "dataset = Dataset.from_list(news)  # Convert list to Dataset\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n------------Original-------------\")\n",
    "print(first_english_sentence)\n",
    "\n",
    "print(\"------------Encoded-------------\")\n",
    "print(tokenized_output['input_ids'])\n",
    "\n",
    "print(\"------------Decoded-------------\")\n",
    "print(tokenizer.decode(tokenized_output['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c475d0-4dd1-4a30-92c4-c4218b6bf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "    import datasets\n",
    "    import evaluate\n",
    "    import accelerate\n",
    "except ImportError:\n",
    "    !pip install transformers datasets evaluate accelerate\n",
    "    !pip install datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52852be3-9c97-4cae-90a1-09c70e34ec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2902a212-4e30-41a8-803e-7d25da26e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'english', 'spanish', 'id_english', 'id_spanish', 'segment_id', 'alignment_id']\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert list of dictionaries into a Dataset\n",
    "dataset = Dataset.from_list(books)\n",
    "\n",
    "# Now you can access column names\n",
    "print(dataset.column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb1f32e-c266-4662-a092-09219262d31f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_books' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[0;32m----> 6\u001b[0m split_dataset \u001b[38;5;241m=\u001b[39m tokenized_books[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a new DatasetDict with the train-test split\u001b[39;00m\n\u001b[1;32m      9\u001b[0m books \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: split_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: split_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_books' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "split_dataset = tokenized_books['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Create a new DatasetDict with the train-test split\n",
    "books = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'test': split_dataset['test']\n",
    "})\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    books['train'].remove_columns([\"english\", \"spanish\"]),  \n",
    "    shuffle=True,\n",
    "    batch_size=256,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    books['test'].remove_columns([\"english\", \"spanish\"]), \n",
    "    shuffle=False,\n",
    "    batch_size=256,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(\"Train DataLoader ready with batch size:\", train_dataloader.batch_size)\n",
    "print(\"Test DataLoader ready with batch size:\", test_dataloader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ca0e4-aee8-4752-ab5d-ccec9d68f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4e7ec-83f7-4bdc-af28-0b54dbf41701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad0b54-becd-42f9-9238-fbb304e92011",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad233d-1bb9-4ad4-bc09-f8f9251ecec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_dim,pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_index)        \n",
    "        self.rnn = nn.LSTM(embed_dim, embed_dim, batch_first=True,dropout=0.0)\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedding=self.embedding(x)\n",
    "        output, states = self.rnn(embedding)\n",
    "        output=self.fc(output[:,-1,:]).squeeze()\n",
    "        return torch.nn.functional.sigmoid(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a88782-5b36-4d57-91d4-b3034fbcae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())  # Check if 'labels' is in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765cf63-4c8f-4134-b6f2-39506caf4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import Accuracy, F1Score\n",
    "\n",
    "n_epochs = 1\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "model = SimpleRNNClassifier(vocab_size=vocab_size, embed_dim=50, output_dim=1, pad_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Metrics initialization\n",
    "accuracy_metric = Accuracy()\n",
    "f1_metric = F1Score()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    _loss = []\n",
    "    \n",
    "    # Training Loop\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch['input_ids'])\n",
    "        loss = loss_fn(output, batch['labels'].float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _loss.append(loss.item())\n",
    "    \n",
    "    # Save the last 10 training losses\n",
    "    train_loss.append(np.mean(_loss[-10:]))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Evaluation Loop\n",
    "        for test_batch in test_dataloader:\n",
    "            test_output = model(test_batch['input_ids'])\n",
    "            test_label = test_batch['labels'].float()\n",
    "            loss = loss_fn(test_output, test_label)\n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "            # Store predictions and true labels for metrics\n",
    "            all_preds.append(test_output)\n",
    "            all_labels.append(test_label)\n",
    "\n",
    "        # Concatenate all predictions and labels\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        \n",
    "        # Calculate accuracy and F1 score\n",
    "        accuracy = accuracy_metric(all_preds.round(), all_labels)\n",
    "        f1 = f1_metric(all_preds.round(), all_labels)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {train_loss[-1]:.4f} - Val Loss: {np.mean(test_loss[-len(test_dataloader):]):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062d31c-bde7-4b53-a048-49112207d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = \"/Users/mayarios/Desktop/deeplearning/project/cleaned_books_data.csv\"\n",
    "news = \"/Users/mayarios/Desktop/deeplearning/project/cleaned_news_data.csv\"\n",
    "\n",
    "books_df = pd.read_csv(books)\n",
    "news_df = pd.read_csv(news)\n",
    "\n",
    "print(\"First 5 rows of the books CSV file:\")\n",
    "print(books_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the news CSV file:\")\n",
    "print(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b335e-224b-4e2b-a806-71a3274bbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9f91a-2041-4f09-9aca-168936c493a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define the file path\n",
    "books_path = \"/Users/mayarios/Desktop/Deep Learning/potential project/cleaned_books_data.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(books_path, 'r') as f:\n",
    "    books = json.load(f)\n",
    "\n",
    "# Print the first few entries\n",
    "print(\"First few entries in the JSON file:\")\n",
    "print(json.dumps(books[:5], indent=4))\n",
    "\n",
    "# Access specific fields\n",
    "first_english_sentence = books[0]['english']\n",
    "first_spanish_sentence = books[0]['spanish']\n",
    "\n",
    "print(\"\\nFirst English sentence:\", first_english_sentence)\n",
    "print(\"First Spanish sentence:\", first_spanish_sentence)\n",
    "\n",
    "# Tokenize the first English sentence\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "output = tokenizer(first_english_sentence, truncation=True, max_length=100)\n",
    "\n",
    "print(\"\\n------------Original-------------\")\n",
    "print(first_english_sentence)\n",
    "\n",
    "print(\"------------Encoded-------------\")\n",
    "print(output['input_ids'])\n",
    "\n",
    "print(\"------------Decoded-------------\")\n",
    "print(tokenizer.decode(output['input_ids']))\n",
    "\n",
    "# Save a subset of the data\n",
    "subset = books[:10]  # Get the first 10 entries\n",
    "with open(\"subset_books.json\", \"w\") as f:\n",
    "    json.dump(subset, f, indent=4)\n",
    "\n",
    "print(\"\\nSubset saved to 'subset_books.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616fa85-4c82-4341-96fa-195a4579df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_books = books.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3853268-5f2c-4ecb-bbc1-7891d889f8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
